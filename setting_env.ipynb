{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM0K3i05+77gNceOshllCcG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jin1402/DSA4262/blob/main/setting_env.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pls run on T4 GPT"
      ],
      "metadata": {
        "id": "09d1nJF0FCqv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 1 â€“ Environment Setup & Reproducibility"
      ],
      "metadata": {
        "id": "y2st0btBFu34"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project evaluates the Dreaddit dataset for stress detection. We specifically consider the implications of deploying such a tool in a Singaporean digital mental health context (e.g., monitoring community support forums or wellness apps). We focus on the trade-off between model complexity (DistilBERT) and human-interpretable signals (Logistic Regression)."
      ],
      "metadata": {
        "id": "pWp5u1swU_2E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSfXo1dQFeU2"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q scikit-learn pandas numpy matplotlib seaborn nltk shap transformers torch tqdm datasets accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "uzkeOrGfFopN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reproducibility\n",
        "SEED = 32 # my fav number\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n"
      ],
      "metadata": {
        "id": "nG-BVs5UFt_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 2 â€“ Data Loading"
      ],
      "metadata": {
        "id": "zUqw_Fq5F3qV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "p39tSOGfG4mF",
        "outputId": "8123555d-1e0b-4da8-8ae7-23613e811da7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = \"/content/drive/MyDrive/dreaddit/\"\n",
        "\n",
        "train_df = pd.read_csv(DATA_PATH + \"dreaddit-train.csv\")\n",
        "test_df = pd.read_csv(DATA_PATH + \"dreaddit-test.csv\")\n",
        "\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "RIutNVZFG_69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.shape[0]/(train_df.shape[0]+test_df.shape[0]))"
      ],
      "metadata": {
        "id": "eLwhKVDKcZ2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset train-test ratio is 80/20, the standard practice"
      ],
      "metadata": {
        "id": "7AJ3xk8efDKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic sanity checks\n",
        "print(train_df.shape)\n",
        "print(train_df['label'].value_counts())"
      ],
      "metadata": {
        "id": "lGe4mIwQHCH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for any missing values across the entire dataset\n",
        "total_missing = train_df.isnull().sum().sum()\n",
        "if total_missing > 0:\n",
        "    print(f\"âš ï¸ Warning: There are {total_missing} missing values in the dataset.\")\n",
        "    # Optional: show which columns have them\n",
        "    print(\"\\nColumns with missing values:\")\n",
        "    print(train_df.isnull().sum()[train_df.isnull().sum() > 0])\n",
        "else:\n",
        "    print(\"âœ… Clean Data: No missing values found in any of the columns.\")\n",
        "\n",
        "# Quick statistical check of the precomputed sentiment\n",
        "# Group by label and calculate both mean and standard deviation\n",
        "sentiment_stats = train_df.groupby('label')['sentiment'].agg(['mean', 'std'])\n",
        "print(\"Sentiment Statistics by Class (1=Stress, 0=Non-Stress):\")\n",
        "print(sentiment_stats)\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(\n",
        "    data=train_df,\n",
        "    x='label',\n",
        "    y='sentiment',\n",
        "    hue='label',      # ADD THIS: Link color to the label\n",
        "    legend=False,     # ADD THIS: Removes the redundant legend\n",
        "    palette=\"coolwarm\"\n",
        ")\n",
        "\n",
        "# Adding labels for the grader to see your \"Sense-making\"\n",
        "plt.title('Distribution of Sentiment Scores')\n",
        "plt.xlabel('Label (0: Non-Stress, 1: Stress)')\n",
        "plt.ylabel('Precomputed Sentiment Score')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FxPm-Vkjk7FF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Overlap:** While stressed posts have a lower average sentiment, as the \"boxes\" overlap significantly, it means sentiment alone is a weak predictor.\n",
        "\n",
        "\n",
        "**The Outliers:**\n",
        "1. The \"Resilient\" Outliers (Stressed posts with High Sentiment)\n",
        "These represent posts where a user is technically in distress but uses highly positive words.\n",
        "\n",
        "-> This is often a sign of sarcasm or social masking.\n",
        "\n",
        "*   Example 1:\n",
        "*   Example 2:\n",
        "\n",
        "-> Model Risk: The model might misclassify these as \"Non-Stress\" if it relies too heavily on sentiment alone.\n",
        "\n",
        "2. The \"Stoic\" Outliers (Non-Stressed posts with Low Sentiment)\n",
        "These users aren't \"stressed\" according to the dataset, yet they use negative language.\n",
        "\n",
        "->These are likely posts discussing objective negative events or providing advice in a serious tone\n",
        "\n",
        "*   Example 1:\n",
        "*   Example 2:\n",
        "\n",
        "-> Model Risk: These are prone to False Positives.\n",
        "\n",
        "3. Asymmetry in Outlier Density\n",
        "There are significantly more outliers in the Non-Stress (0) category than in the Stress (1) category.\n",
        "\n",
        "-> Non-stressed communication on Reddit is linguistically more diverse. People use a wider range of \"extreme\" sentiment (both very happy and very sad) without it necessarily being tied to personal clinical stress.\n",
        "\n",
        "-> Stress (in this dataset) tends to pull sentiment toward a specific \"neutral-to-negative\" range, making the data for Class 1 more \"concentrated\" than Class 0."
      ],
      "metadata": {
        "id": "PpUJDy69nolt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 3 â€“ Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "3mT8Vy5CHSEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Class balance\n",
        "train_df['label'].value_counts().plot(kind='bar', title='Class Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VZUYlQpvHE3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Calculate the order (from most samples to least) for a cleaner chart\n",
        "subreddit_order = train_df['subreddit'].value_counts().index\n",
        "\n",
        "# 2. Create the plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(\n",
        "    data=train_df,\n",
        "    y='subreddit',\n",
        "    order=subreddit_order,\n",
        "    hue='subreddit',      # Assign hue to subreddit to avoid the FutureWarning\n",
        "    legend=False,         # Hide the redundant legend\n",
        "    palette='viridis'\n",
        ")\n",
        "\n",
        "# 3. Add labels and title\n",
        "plt.title('Distribution of Samples across Subreddits', fontsize=14)\n",
        "plt.xlabel('Number of Posts', fontsize=12)\n",
        "plt.ylabel('Subreddit', fontsize=12)\n",
        "\n",
        "# 4. Add gridlines for readability\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.tight_layout() # Ensures labels aren't truncated\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GnIawje1qu8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Domain Representation & Context Trap:**\n",
        "\n",
        "The dataset provides a diverse mix of social, practical, and clinical stress contexts, but they are not distributed equally.\n",
        "\n",
        "- The high representation of subreddits like r/ptsd, r/relationships and r/anxiety suggests the model will be well-trained on ***clinical symptoms***, but may struggle with ***'situational' stress*** found in smaller groups like r/almosthomeless or r/food_pantry.\n",
        "\n",
        "**Dataset Balance:**\n",
        "\n",
        "There is a significant imbalance across subredditsâ€”ptsd has nearly 600 samples, while food_pantry has fewer than 50.\n",
        "\n",
        "That most likely leads to:\n",
        "\n",
        "- Risk of Majority Bias: The model will naturally become an \"expert\" at identifying stress as it is expressed in r/ptsd and r/relationships because that is where the bulk of the data comes from.\n",
        "\n",
        "- Performance Disparity: You can hypothesize that the model will likely perform poorly on r/food_pantry or r/stress simply because it hasn't seen enough examples of the specific linguistic \"flavor\" of those communities."
      ],
      "metadata": {
        "id": "_25of7GxrF1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stress rate by subreddit\n",
        "subreddit_stats = train_df.groupby('subreddit')['label'].mean().sort_values()\n",
        "\n",
        "subreddit_stats.plot(kind='barh', figsize=(6,8), title='Stress Rate by Subreddit')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "todz7zQuHWXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if stressed users write more\n",
        "train_df['text_len'] = train_df['text'].str.split().str.len()\n",
        "train_df.groupby('label')['text_len'].mean().plot(kind='bar', title='Avg Post Length by Class')\n",
        "plt.ylabel('Word Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MpcilWoZVL9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Length of stressed posts and other posts are similar, so we can say for now that the model does not just be learning to detect 'verbosity' rather than 'stress.'\n",
        "\n",
        "**Observations**\n",
        "- Some subreddits show significantly higher stress prevalence.\n",
        "- This suggests community norms and posting behavior may affect predictability.\n"
      ],
      "metadata": {
        "id": "Oyvm5oOZHbSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def get_top_words(df, label, n=20):\n",
        "    words = \" \".join(df[df['label'] == label]['text']).split()\n",
        "    # Filter out stop words and punctuation\n",
        "    filtered_words = [w for w in words if w not in stop_words and w.isalpha()]\n",
        "    return Counter(filtered_words).most_common(n)\n",
        "\n",
        "# Get the top words for both classes\n",
        "top_stress = get_top_words(train_df, 1, n=20)\n",
        "top_non_stress = get_top_words(train_df, 0, n=20)\n",
        "\n",
        "# Convert to a DataFrame for a clean table view\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Stress Word': [word for word, count in top_stress],\n",
        "    'Stress Count': [count for word, count in top_stress],\n",
        "    'Non-Stress Word': [word for word, count in top_non_stress],\n",
        "    'Non-Stress Count': [count for word, count in top_non_stress]\n",
        "})\n",
        "\n",
        "# Display the table\n",
        "print(\"Table 1: Top 20 Most Frequent Words by Class\")\n",
        "display(comparison_df)"
      ],
      "metadata": {
        "id": "mG3B9--5d4q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The top words of both classes are neutral"
      ],
      "metadata": {
        "id": "Mcb0_iwHezOY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 4 â€“ Text Preprocessing & Features"
      ],
      "metadata": {
        "id": "oIa69piVHgUh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Cleaning & Minimal Normalization\n",
        "1. Selective Regex Cleaning (The \"Noise\" Layer):\n",
        "- Remove URLs (Reddit posts are full of these)\n",
        "- Remove Reddit Mentions (u/user) and Subreddits (r/sub)\n",
        "- Remove extra whitespace\n",
        "- Remove non-ASCII characters\n",
        "2. Standardizing via Lowercasing\n",
        "3. Avoiding Lemmatization or Stemming (The \"Signal\" Layer): It is important to retain the original tense and intensity of words, which are critical to distinguish between active distress and historical reporting\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G0MuP6nkmWrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def clean_text(text):\n",
        "    # 1. Lowercase\n",
        "    text = text.lower()\n",
        "    # 2. Remove URLs (Reddit posts are full of these)\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # 3. Remove Reddit Mentions (u/user) and Subreddits (r/sub)\n",
        "    text = re.sub(r'@[^\\s]+|u/[^\\s]+|r/[^\\s]+', '', text)\n",
        "    # 4. Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # 5. Remove non-ASCII characters\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
        "    return text\n",
        "\n",
        "# Apply to your dataframe\n",
        "train_df['cleaned_text'] = train_df['text'].apply(clean_text)"
      ],
      "metadata": {
        "id": "jBrUswEThSqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df['cleaned_text'][63])"
      ],
      "metadata": {
        "id": "rbHCAKJFhhS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "It rewards words that are frequent in one post but punishes words that are common across every post (like \"the\" or \"Reddit\")."
      ],
      "metadata": {
        "id": "2YFdPPXhmRcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction import text\n",
        "\n",
        "stop_words = list(text.ENGLISH_STOP_WORDS) # Removes common \"filler\" words (e.g., the, is, at)\n",
        "stop_words.extend(['url', 'link', 'http', 'https', 'com']) # customised list\n",
        "\n",
        "print(stop_words)"
      ],
      "metadata": {
        "id": "8vK7ZoRjhZbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_text = train_df['text']\n",
        "X_text_cleaned = train_df['text'].apply(clean_text)\n",
        "y = train_df['label']\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_text, y, test_size=0.2, stratify=y, random_state=SEED\n",
        ")"
      ],
      "metadata": {
        "id": "jd7yC_5VHfk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use Stratified splitting to ensure that the distribution of subreddits and the stress-label ratio remains consistent across our training and validation sets, preventing 'data luck' from skewing results."
      ],
      "metadata": {
        "id": "UoNRDWvMVyQf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 5 â€“ Baseline Model: Logistic Regression"
      ],
      "metadata": {
        "id": "cEW6o-jTHquB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Term Frequency-Inverse Document Frequency"
      ],
      "metadata": {
        "id": "eUz58sxNqTG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "\n",
        "# 1. Initialize K-Fold\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "\n",
        "best_f1 = 0\n",
        "best_model = None\n",
        "best_tfidf = None\n",
        "\n",
        "# Track all three metrics\n",
        "fold_f1_scores = []\n",
        "fold_precision_scores = []\n",
        "fold_recall_scores = []\n",
        "fold = 1\n",
        "\n",
        "for train_index, val_index in skf.split(X_text, y):\n",
        "    # Split the data\n",
        "    X_train_fold, X_val_fold = X_text_cleaned.iloc[train_index], X_text_cleaned.iloc[val_index]\n",
        "    y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n",
        "\n",
        "    # Vectorize inside the fold (Prevents Data Leakage)\n",
        "    tfidf_fold = TfidfVectorizer(\n",
        "        lowercase=True, # treat capitalised and non-capitalised word as one\n",
        "        stop_words=stop_words, # defined earlier\n",
        "        ngram_range=(1, 2), # Unigrams and Bigrams (ie. capture words as \"alone\" or \"in pair\"), the second number can be increased to capture longer phrases)\n",
        "        max_features=10000   # Limit vocab size\n",
        "    )\n",
        "    X_train_vec = tfidf_fold.fit_transform(X_train_fold)\n",
        "    X_val_vec = tfidf_fold.transform(X_val_fold)\n",
        "\n",
        "    # Train\n",
        "    model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "    model.fit(X_train_vec, y_train_fold)\n",
        "\n",
        "    # Evaluate\n",
        "    preds = model.predict(X_val_vec)\n",
        "\n",
        "    # Calculate all metrics\n",
        "    current_f1 = f1_score(y_val_fold, preds)\n",
        "    current_prec = precision_score(y_val_fold, preds)\n",
        "    current_rec = recall_score(y_val_fold, preds)\n",
        "\n",
        "    fold_f1_scores.append(current_f1)\n",
        "    fold_precision_scores.append(current_prec)\n",
        "    fold_recall_scores.append(current_rec)\n",
        "\n",
        "    print(f\"Fold {fold} | F1: {current_f1:.4f} | Prec: {current_prec:.4f} | Rec: {current_rec:.4f}\")\n",
        "    fold += 1\n",
        "\n",
        "    # Save the best one based on F1-Score\n",
        "    if current_f1 > best_f1:\n",
        "        best_f1 = current_f1\n",
        "        best_model = model\n",
        "        best_tfidf = tfidf_fold\n",
        "\n",
        "print(f\"\\n--- Final CV Results ---\")\n",
        "print(f\"Average F1-Score:  {np.mean(fold_f1_scores):.4f} (+/- {np.std(fold_f1_scores):.4f})\")\n",
        "print(f\"Average Precision: {np.mean(fold_precision_scores):.4f}\")\n",
        "print(f\"Average Recall:    {np.mean(fold_recall_scores):.4f}\")\n",
        "\n",
        "# VIsualisation\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. IMPORTANT: Use the best_tfidf to transform your validation text\n",
        "# We use the X_val_fold from the last iteration of your loop,\n",
        "# or use your original X_test if you are ready for final evaluation.\n",
        "X_val_vec_best = best_tfidf.transform(X_val_fold)\n",
        "\n",
        "# 2. Generate predictions using the best_model\n",
        "y_pred_best = best_model.predict(X_val_vec_best)\n",
        "\n",
        "# 3. Create the confusion matrix\n",
        "cm = confusion_matrix(y_val_fold, y_pred_best)\n",
        "\n",
        "# 4. Plot the matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "disp = ConfusionMatrixDisplay(\n",
        "    confusion_matrix=cm,\n",
        "    display_labels=['Non-Stress (0)', 'Stress (1)']\n",
        ")\n",
        "disp.plot(cmap='Blues', values_format='d')\n",
        "plt.title(f\"Confusion Matrix: Best Fold (F1: {best_f1:.4f})\", fontsize=14)\n",
        "plt.grid(False) # Clean up the background\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N3MHzbeUqjC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Baseline Performance Analysis:\n",
        "The model achieved an average F1-score of 0.7534, which is highly non-trivial.\n",
        "\n",
        "**Robustness**: The near-equal Precision (0.7514) and Recall (0.7560) indicate the model is well-balanced and successfully avoids the \"majority class bias\" often seen in imbalanced social media data.\n",
        "\n",
        "**Stability**: The low standard deviation (0.02) across 5 folds confirms that the linguistic patterns the model has learned (via TF-IDF bigrams) are consistent across the different online communities in the dataset."
      ],
      "metadata": {
        "id": "xKi2QkTUsH7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Prepare data\n",
        "feature_names = best_tfidf.get_feature_names_out()\n",
        "coefficients = best_model.coef_[0]\n",
        "coef_df = pd.DataFrame({'Word': feature_names, 'Weight': coefficients})\n",
        "top_stress = coef_df.sort_values(by='Weight', ascending=False).head(15)\n",
        "top_non_stress = coef_df.sort_values(by='Weight', ascending=True).head(15)\n",
        "combined_df = pd.concat([top_stress, top_non_stress]).sort_values(by='Weight')\n",
        "\n",
        "# 2. Plotting with 'coolwarm'\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(\n",
        "    data=combined_df,\n",
        "    x='Weight',\n",
        "    y='Word',\n",
        "    palette='coolwarm'\n",
        ")\n",
        "\n",
        "# 3. Add the vertical zero-line\n",
        "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.8, alpha=0.5)\n",
        "\n",
        "# 4. Add the explicit labels you requested\n",
        "# We place these at the top of the chart area\n",
        "plt.text(combined_df['Weight'].min(), -1, \"â† Predicts NON-STRESS\",\n",
        "         color='#4575b4', fontweight='bold', fontsize=12, ha='left')\n",
        "plt.text(combined_df['Weight'].max(), -1, \"Predicts STRESS â†’\",\n",
        "         color='#d73027', fontweight='bold', fontsize=12, ha='right')\n",
        "\n",
        "# 5. Styling\n",
        "plt.title(\"Top 15 Stress vs. Non-Stress Predictors\", fontsize=15, pad=30)\n",
        "plt.xlabel(\"Coefficient Weight\", fontsize=12)\n",
        "plt.ylabel(\"\")\n",
        "sns.despine(left=True, bottom=True)\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.4)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bgEluGccf9Il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation on Top Predictors:**\n",
        "\n",
        "The highest-weighted features are functional words like \"don know\" and \"feel\". While these are not clinical symptoms themselves, they capture the linguistic style of distress: specifically, the use of negations to express helplessness (\"don\") and internal-state verbs (\"feel\") to express rumination. The fact that these outrank explicit keywords like \"anxiety\" suggests the model is successfully learning the contextual tone of the posts, not just keyword-matching.\n",
        "\n",
        "While the top stress predictors were dominated by internalizing verbs ('feel') and negations ('don know'), the non-stress predictors are characterized by positive social markers ('thank', 'nice') and outward-facing communication ('survey', 'let know'). This confirms that the model is successfully distinguishing between the 'inward-looking' language of distress and the 'outward-looking' or 'socially-normative' language of standard Reddit interactions."
      ],
      "metadata": {
        "id": "3YBzSWxqW3Ul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Local Interpretation"
      ],
      "metadata": {
        "id": "70Qy2SwAiIQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now zoom into some example to see where the model succeeds and/or fails"
      ],
      "metadata": {
        "id": "qTDZgNXknRRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Case 1: True Negative\n",
        "\n",
        "The high score for TN proves the model isn't just flagging everything as Stress"
      ],
      "metadata": {
        "id": "ivx2nesNmvLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a DataFrame of Validation results\n",
        "val_df = pd.DataFrame({\n",
        "    'post_id': X_val_fold.index,\n",
        "    'text': X_val_fold,\n",
        "    'actual': y_val_fold,\n",
        "    'pred': y_pred_best,\n",
        "    'prob_stress': best_model.predict_proba(X_val_vec_best)[:, 1]\n",
        "})\n",
        "\n",
        "# 2. Filter for True Negatives (Model correctly said Non-Stress)\n",
        "# We sort by 'prob_stress' ascending to find the \"Least Stressed\" posts\n",
        "true_negatives = val_df[(val_df['actual'] == 0) & (val_df['pred'] == 0)].sort_values(by='prob_stress')\n",
        "\n",
        "# 3. Display the top 3 clearest examples of Non-Stress\n",
        "print(\"Top 3 Confident Non-Stress Posts:\")\n",
        "for i in range(3):\n",
        "    print(f\"\\nPost ID: {true_negatives.iloc[i]['post_id']} (Stress Prob: {true_negatives.iloc[i]['prob_stress']:.4f}):\")\n",
        "    print(true_negatives.iloc[i]['text'])"
      ],
      "metadata": {
        "id": "qL5YF2IUiD3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MANUAL MAPPING OF TOP 15 PREDICTION WORDS**\n",
        "\n",
        "**Post ID:** 259\n",
        "\n",
        "**Model Confidence:** 86.96%\n",
        "\n",
        "| Token Found | Global Rank (Day 5) | Direction | Clinical / Contextual Reason |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **\"met\"** | #1 Non-Stress | ðŸ”µ Non-Stress | The strongest \"safe\" indicator; relates to external social events. |\n",
        "| **\"survey\"** | #3 Non-Stress | ðŸ”µ Non-Stress | High-weight marker for research/data collection vs. personal venting. |\n",
        "| **\"free\"** | #4 Non-Stress | ðŸ”µ Non-Stress | Indicates choice/agency; used here in \"free to leave a response.\" |\n",
        "| **\"feel\"** | #2 Stress | ðŸ”´ Stress | Heavy stress marker globally, but overridden by Non-Stress context here. |\n",
        "| **\"don\"** | #3 Stress | ðŸ”´ Stress | Fragment of \"don't\" (from \"don't know\"); indicates uncertainty. |\n",
        "\n",
        "**Interpretability Analysis:** This post is a fascinating \"tug-of-war.\" It contains two of your top-3 **Stress** predictors (**\"feel\"** and **\"don\"**), yet the model remains highly confident in a **Non-Stress** label. This is because the post also contains the #1 (**\"met\"**), #3 (**\"survey\"**), and #4 (**\"free\"**) **Non-Stress** predictors. The combined mathematical weight of these three \"Blue\" tokens successfully neutralized the \"Red\" tokens, proving the model is looking at the collective evidence rather than just a single keyword."
      ],
      "metadata": {
        "id": "iOYB5WVyjPsk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Case 2: False Negative"
      ],
      "metadata": {
        "id": "3nP-pDq6oAKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Filter for False Negatives (Actual: 1, Pred: 0)\n",
        "# Sorting by prob_stress ascending shows where the model missed stress entirely\n",
        "false_negatives = val_df[(val_df['actual'] == 1) & (val_df['pred'] == 0)].sort_values(by='prob_stress', ascending=True)\n",
        "\n",
        "# 2. Display the top 3 \"missed\" stress posts\n",
        "print(\"Top 3 False Negatives (Model missed the Stress):\")\n",
        "for i in range(min(3, len(false_negatives))):\n",
        "    row = false_negatives.iloc[i]\n",
        "    print(f\"\\nPost ID: {row['post_id']} (Stress Prob: {row['prob_stress']:.4f}):\")\n",
        "    print(row['text'])"
      ],
      "metadata": {
        "id": "3KXHnZtSmRY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MANUAL MAPPING OF TOP 15 PREDICTION WORDS**\n",
        "\n",
        "**Post ID:** 1950\n",
        "\n",
        "**Model Confidence:** 66.01%\n",
        "\n",
        "| Token Found | Global Rank (Day 5) | Direction | Clinical / Contextual Reason |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **\"thank\"** | #2 Non-Stress | ðŸ”µ Non-Stress | Used twice; social gratitude is a massive \"safe\" signal for the model. |\n",
        "| **\"helped\"** | #13 Non-Stress | ðŸ”µ Non-Stress | Used in \"it helps\"; the model associates this with recovery or assistance. |\n",
        "| **\"just\"** | #1 Stress | ðŸ”´ Stress | Present here (\"just needing\"), but its weight is neutralized by the \"Blue\" tokens. |\n",
        "| **\"feeling\"** | #4 Stress | ðŸ”´ Stress | (Represented by \"hypersensitivity/sensitivity\") Clinical markers for high-stress states. |\n",
        "| **\"bad\"** | #16 Stress | ðŸ”´ Stress | (Represented by \"it blows\") Negative valence that indicates the actual stress label. |\n",
        "\n",
        "**Interpretability Analysis:** This is a **False Negative** where the model's reliance on \"pro-social\" markers led to an error. Even though the user describes severe symptoms like **\"hypervigilance\"** and **\"catastrophizing,\"** the model is overwhelmed by the presence of **\"thank\"** (Rank #2 Non-Stress) used twice and **\"helps\"** (Rank #13 Non-Stress). The model interprets the user's politeness as a sign of stability, failing to recognize that a person can be in a \"shame spiral\" while still maintaining social norms. This demonstrates that a Bag-of-Words approach can be \"fooled\" by a few high-weight polite tokens."
      ],
      "metadata": {
        "id": "MuBH2D09qI6F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Case 3: False Positive\n",
        "\n"
      ],
      "metadata": {
        "id": "8LDMz5lYno6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Filter for False Positives (Actual: 0, Pred: 1)\n",
        "# Sorting by prob_stress descending shows where the model was WRONG and CONFIDENT\n",
        "false_positives = val_df[(val_df['actual'] == 0) & (val_df['pred'] == 1)].sort_values(by='prob_stress', ascending=False)\n",
        "\n",
        "# 2. Display the top 3 most \"confusing\" posts for the model\n",
        "print(\"Top 3 False Positives (Model was 'Fooled'):\")\n",
        "for i in range(min(3, len(false_positives))):\n",
        "    row = false_positives.iloc[i]\n",
        "    print(f\"\\nPost ID: {row['post_id']} (Stress Prob: {row['prob_stress']:.4f}):\")\n",
        "    print(row['text'])\n"
      ],
      "metadata": {
        "id": "ltt7RqDPn1M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MANUAL MAPPING OF TOP 15 PREDICTION WORDS**\n",
        "\n",
        "**Post ID:** 1828\n",
        "\n",
        "**Model Confidence:** 88.19%\n",
        "\n",
        "| Token Found | Global Rank (Day 5) | Direction | Clinical / Contextual Reason |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **\"feel\"** | #2 Stress | ðŸ”´ Stress | Used 5 times; the model incorrectly interprets \"can't feel\" as emotional rumination. |\n",
        "| **\"don know\"**| #4 Stress | ðŸ”´ Stress | Flagged as a lack of agency, though here it is used to express factual uncertainty. |\n",
        "| **\"just\"** | #1 Stress | ðŸ”´ Stress | Heavy weight as a filler word, appearing here in \"just... i don't know\". |\n",
        "| **\"anxiety\"**| #6 Stress | ðŸ”´ Stress | The model triggers on \"anxious states\" without realizing the user is distancing themselves from the term. |\n",
        "| **\"bad\"** | #15 Stress | ðŸ”´ Stress | Contributes to the negative weight despite being used in a descriptive context. |\n",
        "\n",
        "**Interpretability Analysis:** This is a **False Positive** error that exposes the \"Bag-of-Words\" limitation. While the user explicitly states they **\"can't feel\"** and that their trauma **\"doesn't feel like anything disturbing,\"** the model is unable to process the negation (\"can't\" or \"doesn't\"). It simply sees the high density of top-tier stress tokens like **\"feel\"** (#2) and **\"don know\"** (#4) and mathematically concludes this is a stress state. This proves that while the model is statistically accurate, it lacks the semantic depth to understand when a user is clinically describing their symptoms rather than experiencing them."
      ],
      "metadata": {
        "id": "crosPeXlopuJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 6 â€“ TF-IDF and LIWC Hybrid Model"
      ],
      "metadata": {
        "id": "vb-HmRYVTEJO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1. Hybrid Feature Construction**\n",
        "\n",
        "To move beyond the baseline, we developed a Hybrid Feature Set. This combined the statistical strength of TF-IDF (Local Context) with the psychological depth of LIWC (Global Context).\n",
        "\n",
        "*   TF-IDF: Captured specific Reddit-based markers like \"just\" and \"don know\".\n",
        "\n",
        "*   LIWC/Sentiment: Provided categories like lex_liwc_focuspresent (Present Tense) and lex_liwc_i (Self-reference) to capture the user's psychological state.\n",
        "\n",
        "**Step 2. Feature Scaling & Normalization**\n",
        "\n",
        "A critical step in our methodology was the use of the MinMaxScaler.\n",
        "\n",
        "*   Reasoning: TF-IDF values naturally fall between 0 and 1. However, raw LIWC counts (e.g. lex_liwc_Clout) can be much higher. Without scaling, the model would incorrectly assume a LIWC score of 50 is \"50 times more important\" than a TF-IDF score of 1.0.\n",
        "\n",
        "*   Action: We compressed all numerical features into the [0, 1] range to ensure a \"fair fight\" between text and metadata during the optimization process.\n",
        "\n",
        "**Step 3. Integrated Cross-Validation Pipeline**\n",
        "\n",
        "To ensure the integrity of our results, we performed Retraining inside a 5-Fold Stratified Cross-Validation loop.\n",
        "\n",
        "*   Leakage Prevention: The TfidfVectorizer was fitted only on the training folds for each iteration.\n",
        "\n",
        "*   Horizontal Stacking: We used hstack to join the sparse TF-IDF matrix with the dense LIWC array, creating a unified input vector."
      ],
      "metadata": {
        "id": "vPjdGnMw1xVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy.sparse import hstack\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- 1. NEW: PREPARE LIWC FEATURES BEFORE THE LOOP ---\n",
        "liwc_cols = [col for col in train_df.columns if col.startswith('lex_')]\n",
        "# Scale to [0, 1] range\n",
        "scaler = MinMaxScaler()\n",
        "X_liwc_scaled = scaler.fit_transform(train_df[liwc_cols])\n",
        "\n",
        "# --- 2. INITIALIZE K-FOLD ---\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "\n",
        "best_f1 = 0\n",
        "best_model = None\n",
        "best_tfidf = None\n",
        "best_val_index = None # NEW: Track the best fold's validation data\n",
        "\n",
        "# Track all three metrics\n",
        "fold_f1_scores = []\n",
        "fold_precision_scores = []\n",
        "fold_recall_scores = []\n",
        "fold = 1\n",
        "\n",
        "# Note: Changed skf.split(X_text, y) to skf.split(X_text_cleaned, y) for consistency\n",
        "for train_index, val_index in skf.split(X_text_cleaned, y):\n",
        "    # Split the TEXT data\n",
        "    X_train_fold = X_text_cleaned.iloc[train_index]\n",
        "    X_val_fold = X_text_cleaned.iloc[val_index]\n",
        "    y_train_fold = y.iloc[train_index]\n",
        "    y_val_fold = y.iloc[val_index]\n",
        "\n",
        "    # --- NEW: Split the LIWC data using the same indices ---\n",
        "    X_liwc_train = X_liwc_scaled[train_index]\n",
        "    X_liwc_val = X_liwc_scaled[val_index]\n",
        "\n",
        "    # Vectorize text inside the fold (Prevents Data Leakage)\n",
        "    tfidf_fold = TfidfVectorizer(\n",
        "        lowercase=True,\n",
        "        stop_words=stop_words,\n",
        "        ngram_range=(1, 2),\n",
        "        max_features=10000\n",
        "    )\n",
        "    X_train_text_vec = tfidf_fold.fit_transform(X_train_fold)\n",
        "    X_val_text_vec = tfidf_fold.transform(X_val_fold)\n",
        "\n",
        "    # --- NEW: Combine TF-IDF and LIWC horizontally ---\n",
        "    X_train_vec = hstack([X_train_text_vec, X_liwc_train])\n",
        "    X_val_vec = hstack([X_val_text_vec, X_liwc_val])\n",
        "\n",
        "    # Train\n",
        "    model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "    model.fit(X_train_vec, y_train_fold)\n",
        "\n",
        "    # Evaluate\n",
        "    preds = model.predict(X_val_vec)\n",
        "\n",
        "    # Calculate all metrics\n",
        "    current_f1 = f1_score(y_val_fold, preds)\n",
        "    current_prec = precision_score(y_val_fold, preds)\n",
        "    current_rec = recall_score(y_val_fold, preds)\n",
        "\n",
        "    fold_f1_scores.append(current_f1)\n",
        "    fold_precision_scores.append(current_prec)\n",
        "    fold_recall_scores.append(current_rec)\n",
        "\n",
        "    print(f\"Fold {fold} | F1: {current_f1:.4f} | Prec: {current_prec:.4f} | Rec: {current_rec:.4f}\")\n",
        "\n",
        "    # Save the best one based on F1-Score\n",
        "    if current_f1 > best_f1:\n",
        "        best_f1 = current_f1\n",
        "        best_model = model\n",
        "        best_tfidf = tfidf_fold\n",
        "        best_val_index = val_index # NEW: Save the index to reconstruct later\n",
        "\n",
        "    fold += 1\n",
        "\n",
        "print(f\"\\n--- Final CV Results ---\")\n",
        "print(f\"Average F1-Score:  {np.mean(fold_f1_scores):.4f} (+/- {np.std(fold_f1_scores):.4f})\")\n",
        "print(f\"Average Precision: {np.mean(fold_precision_scores):.4f}\")\n",
        "print(f\"Average Recall:    {np.mean(fold_recall_scores):.4f}\")\n",
        "\n",
        "# --- VISUALISATION ---\n",
        "\n",
        "# 1. NEW: Reconstruct the exact text and LIWC validation set from the BEST fold\n",
        "X_val_fold_best_text = X_text_cleaned.iloc[best_val_index]\n",
        "y_val_fold_best = y.iloc[best_val_index]\n",
        "X_liwc_val_best = X_liwc_scaled[best_val_index]\n",
        "\n",
        "# 2. Transform the text and combine with LIWC\n",
        "X_val_text_vec_best = best_tfidf.transform(X_val_fold_best_text)\n",
        "X_val_vec_best = hstack([X_val_text_vec_best, X_liwc_val_best])\n",
        "\n",
        "# 3. Generate predictions using the best_model\n",
        "y_pred_best = best_model.predict(X_val_vec_best)\n",
        "\n",
        "# 4. Create the confusion matrix\n",
        "cm = confusion_matrix(y_val_fold_best, y_pred_best)\n",
        "\n",
        "# 5. Plot the matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "disp = ConfusionMatrixDisplay(\n",
        "    confusion_matrix=cm,\n",
        "    display_labels=['Non-Stress (0)', 'Stress (1)']\n",
        ")\n",
        "disp.plot(cmap='Blues', values_format='d')\n",
        "plt.title(f\"Confusion Matrix: Best Fold (F1: {best_f1:.4f})\", fontsize=14)\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WX_2FiFurrpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Engineering Analysis: TF-IDF vs. Hybrid Model**\n",
        "\n",
        "Contrary to our hypothesis, integrating LIWC psychological features with the TF-IDF baseline resulted in an increase in both False Positives and False Negatives, lowering the overall F1-Score. This suggests that for the Dreaddit dataset, the granular, token-level context provided by TF-IDF (e.g., distinguishing the specific use of the word 'just' or 'feel') is more discriminative than the broad psychological categories provided by LIWC.\n",
        "\n",
        "Furthermore, LIWC's predefined dictionaries may lack the contextual nuance required for internet forums, potentially misclassifying colloquial venting or slang as clinical distress. Ultimately, this experiment proves that adding more features does not inherently improve a model if those features abstract away critical local context."
      ],
      "metadata": {
        "id": "YpCnfyYIwPym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Get the feature names from both sources\n",
        "tfidf_feature_names = best_tfidf.get_feature_names_out().tolist()\n",
        "all_feature_names = tfidf_feature_names + liwc_cols\n",
        "\n",
        "# 2. Extract coefficients from the best model\n",
        "# model.coef_[0] gives the weights for all 10,000+ features\n",
        "combined_coef_df = pd.DataFrame({\n",
        "    'Feature': all_feature_names,\n",
        "    'Weight': best_model.coef_[0]\n",
        "})\n",
        "\n",
        "# 3. Create two lists: Top Stress and Top Non-Stress\n",
        "top_combined_stress = combined_coef_df.sort_values(by='Weight', ascending=False).head(20)\n",
        "top_combined_non_stress = combined_coef_df.sort_values(by='Weight', ascending=True).head(20)\n",
        "\n",
        "print(\"Top 20 Features predicting STRESS (Hybrid Model):\")\n",
        "print(top_combined_stress)\n",
        "\n",
        "print(\"\\nTop 20 Features predicting NON-STRESS (Hybrid Model):\")\n",
        "print(top_combined_non_stress)"
      ],
      "metadata": {
        "id": "MwImdfLaxqw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Stress Predictors (ðŸ”´ Red) | Non-Stress Predictors (ðŸ”µ Blue) |\n",
        "| :--- | :--- |\n",
        "| **`lex_liwc_feel`**: Not just the word \"feel\", but a whole family of sensory and emotional tokens. | **`lex_liwc_differ`**: Uses logical operators (e.g., \"but\", \"although\"). Indicates cognitive control over emotional outbursts. |\n",
        "| **`lex_liwc_focuspresent`**: Captures a \"trapped in the now\" state. Stressed users focus on current pain rather than past/future events. | **`lex_liwc_Clout`**: Suggests a confident or authoritative perspective. Aligns with objective reporting rather than personal venting. |\n",
        "| **`lex_liwc_risk`**: Aggregates clinical danger words like \"anxiety\" or \"scared\" into a single high-weight category. | **`lex_dal_min_pleasantness`**: Presence of positive emotional valence. Acts as a strong mathematical counter-indicator of stress. |\n",
        "| **\"don know\" & \"just\"**: Unique Reddit markers. These specific phrases are so discriminative the model refuses to drop them. | **`lex_liwc_focuspast`**: Reflecting on the past. Suggests the user has achieved psychological distance from acute distress. |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kG1BWwl2yL01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why did Performance (F1/FP/FN) Drop?**\n",
        "\n",
        "1.   Overpowering Features: The LIWC features like lex_liwc_Dic have weights as high as 1.72, while the top TF-IDF words are now around 1.25 or lower. The model might be \"leaning\" too hard on these broad categories and losing the precision of the individual words.\n",
        "2.   The \"i\" Bias: lex_liwc_i (the use of \"I\", \"me\", \"my\") is your #3 stress predictor. While high self-reference is a psychological marker of stress, it's also just how people talk on Reddit. The model might be flagging perfectly happy personal stories just because the user used the word \"I\" too much.\n",
        "\n",
        "3.   Conflict between Granularity and Generalization:\n",
        "\n",
        "*   Baseline: Knew that \"survey\" = Non-Stress.\n",
        "*   Hybrid: Might see \"survey\" as part of a \"Work\" or \"Social\" category that isn't as strongly weighted, \"diluting\" the signal.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P5SI0uyK0SNg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**\n",
        "\n",
        "While retraining with LIWC features allowed the model to identify broad psychological patternsâ€”placing lex_liwc_Dic and lex_liwc_feel as top predictorsâ€”the overall performance did not surpass the baseline. This suggests that the raw, unweighted context of specific tokens is the most reliable indicator of stress in the Dreaddit dataset.\n"
      ],
      "metadata": {
        "id": "7qEkcaB82nVW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 7 â€“ Stronger Model: DistilBERT\n",
        "\n",
        "Why DistilBERT?\n",
        "\n",
        "| Challenge in Dreaddit Dataset | Baseline (LR + LIWC) | DistilBERT (Transformer) |\n",
        "| :--- | :--- | :--- |\n",
        "| **Negation Handling** | **Fails**: Triggers on keywords like \"anxiety\" even when negated (e.g., \"not feeling anxiety\"). | **Succeeds**: Uses Attention to link \"not\" to the target emotion, resolving False Positives. |\n",
        "| **Sarcasm & Social Masks** | **Fails**: Literal word counts. Polite words like \"thank\" can mask deep distress (False Negatives). | **Succeeds**: Learns the global \"tone\" of a post, prioritizing clinical markers over social pleasantries. |\n",
        "| **Psychological Insight** | **High**: Directly identifies features like `focuspresent` and `lex_liwc_i` as stress markers. | **Low**: Acts as a \"Black Box\"; provides high accuracy but lacks direct linguistic interpretability. |\n",
        "| **Slang & Typos** | **Low**: Limited to a fixed 10k vocabulary; fails on OOV (Out-of-Vocabulary) slang. | **High**: Uses WordPiece tokenization to break down and understand unseen or misspelled words. |\n",
        "| **Performance (F1)** | **Baseline (75-77%)**: Good for a simple linear model but hits a ceiling due to context blindness. | **State-of-the-Art (85%+)**: Typically provides a 10%+ boost by understanding complex sentence structure. |"
      ],
      "metadata": {
        "id": "4LNeOrP5TNTy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike our baseline model which relied on TF-IDF to create sparse frequency matrices, the DistilBERT implementation utilizes WordPiece tokenization. This allows the model to abandon strict frequency counting in favor of dense contextual embeddings, enabling it to understand word meaning based on surrounding sentence structure."
      ],
      "metadata": {
        "id": "DEkoitcOCk2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base DistilBERT"
      ],
      "metadata": {
        "id": "yTDeIF6pDPg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, IntervalStrategy\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1.1. Reconstruct DataFrames using your cleaned variables\n",
        "# DistilBERT expects a 'text' column and a 'label' column\n",
        "train_df_hf = pd.DataFrame({\n",
        "    'text': X_train.apply(clean_text), # Applying your clean_text function\n",
        "    'label': y_train\n",
        "})\n",
        "\n",
        "val_df_hf = pd.DataFrame({\n",
        "    'text': X_val.apply(clean_text),\n",
        "    'label': y_val\n",
        "})\n",
        "\n",
        "# 1.2. Convert to Hugging Face Dataset format\n",
        "ds_train = Dataset.from_pandas(train_df_hf)\n",
        "ds_val = Dataset.from_pandas(val_df_hf)\n",
        "\n",
        "# 3. DistilBERT uses WordPiece tokenization to handle slang and typos\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Map the tokenization over the datasets\n",
        "tokenized_train = ds_train.map(tokenize_function, batched=True)\n",
        "tokenized_val = ds_val.map(tokenize_function, batched=True)\n",
        "\n",
        "# 3. Initialize Model\n",
        "# We specify 'num_labels=2' for your Stress vs. Non-Stress task\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
        "\n",
        "# 4. Define Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=2e-5,          # Transformers need a very small learning rate\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,          # Usually 3-5 epochs are enough to fine-tune\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=IntervalStrategy.EPOCH, # Calculate accuracy after every epoch\n",
        "    save_strategy=IntervalStrategy.EPOCH,\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# 5. Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        ")\n",
        "\n",
        "# 6. Train (Fine-tune)\n",
        "trainer.train()\n",
        "\n",
        "# 7. Evaluate\n",
        "metrics = trainer.evaluate()\n",
        "print(f\"Final Evaluation Metrics: {metrics}\")"
      ],
      "metadata": {
        "id": "0jtY0H3j5hJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# 1. Define the metric calculator\n",
        "def compute_metrics(eval_pred):\n",
        "    # eval_pred contains the raw model outputs (logits) and the true labels\n",
        "    logits, labels = eval_pred\n",
        "\n",
        "    # Convert logits (raw probabilities) into actual class predictions (0 or 1)\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "\n",
        "    # Return them as a dictionary\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# 2. Update your Trainer to use this function\n",
        "trainer.compute_metrics = compute_metrics\n",
        "\n",
        "# 3. Run evaluation again!\n",
        "detailed_metrics = trainer.evaluate()\n",
        "print(f\"DistilBERT F1-Score: {detailed_metrics['eval_f1']:.4f}\")\n",
        "print(f\"DistilBERT Accuracy: {detailed_metrics['eval_accuracy']:.4f}\")"
      ],
      "metadata": {
        "id": "Ns7TE4udAtlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modified DistilBERT to accept LIWC values"
      ],
      "metadata": {
        "id": "5qiwU8RADTj8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To pair DistilBERT with LIWC, you have to build a Custom Neural Network Architecture. Instead of a straight line, your model becomes shaped like the letter \"Y\".\n",
        "\n",
        "ðŸ§  How the \"Y-Shape\" Architecture Works\n",
        "The Left Branch (Text): Your Reddit text goes through DistilBERT. Instead of predicting \"Stress\" immediately, DistilBERT outputs a Context Vectorâ€”a mathematical summary of the sentence's meaning consisting of 768 numbers.\n",
        "\n",
        "The Right Branch (LIWC): Your scaled numerical LIWC features (e.g., 90 numbers) bypass DistilBERT entirely.\n",
        "\n",
        "The Merger (Concatenation): You glue DistilBERT's 768 numbers and your 90 LIWC numbers together into a single, massive vector of 858 numbers.\n",
        "\n",
        "The Stem (Classification Head): You feed those 858 numbers into a final, standard neural network layer that makes the final \"Stress vs. Non-Stress\" prediction."
      ],
      "metadata": {
        "id": "EeJg5l0LDNf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import DistilBertModel\n",
        "\n",
        "class HybridDistilBertLIWC(nn.Module):\n",
        "    def __init__(self, num_liwc_features, num_labels=2):\n",
        "        super().__init__()\n",
        "        self.distilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.classifier = nn.Linear(768 + num_liwc_features, num_labels)\n",
        "\n",
        "        # --- NEW: Define the mathematical Loss Function ---\n",
        "        self.loss_fct = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, liwc_features, labels=None):\n",
        "        # 1. Get DistilBERT Context Vector\n",
        "        bert_output = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        text_vector = bert_output[0][:, 0]\n",
        "\n",
        "        # 2. Concatenate with LIWC\n",
        "        combined_vector = torch.cat((text_vector, liwc_features), dim=1)\n",
        "\n",
        "        # 3. Get raw predictions (logits)\n",
        "        logits = self.classifier(combined_vector)\n",
        "\n",
        "        # --- NEW: Calculate Loss if labels are provided during training ---\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = self.loss_fct(logits.view(-1, 2), labels.view(-1))\n",
        "\n",
        "        # Hugging Face Trainer REQUIRES this exact dictionary format\n",
        "        return {\"loss\": loss, \"logits\": logits}\n",
        "\n",
        "\n",
        "# Assuming X_liwc_scaled was created for the entire train_df\n",
        "# Use the indices from the initial train_test_split to get the correct LIWC subsets\n",
        "train_indices_initial = X_train.index\n",
        "val_indices_initial = X_val.index\n",
        "\n",
        "# Get the original integer positions from train_df index\n",
        "original_train_df_indices = train_df.index\n",
        "idx_to_pos = {idx: i for i, idx in enumerate(original_train_df_indices)}\n",
        "\n",
        "train_positions = [idx_to_pos[idx] for idx in train_indices_initial]\n",
        "val_positions = [idx_to_pos[idx] for idx in val_indices_initial]\n",
        "\n",
        "X_liwc_train_for_hf = X_liwc_scaled[train_positions]\n",
        "X_liwc_val_for_hf = X_liwc_scaled[val_positions]\n",
        "\n",
        "# 1. Add the LIWC arrays as a new column to the Hugging Face datasets\n",
        "tokenized_train = tokenized_train.add_column(\"liwc_features\", X_liwc_train_for_hf.tolist())\n",
        "tokenized_val = tokenized_val.add_column(\"liwc_features\", X_liwc_val_for_hf.tolist())\n",
        "\n",
        "# 2. Format the datasets specifically for PyTorch tensors\n",
        "tokenized_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'liwc_features', 'label'])\n",
        "tokenized_val.set_format(type='torch', columns=['input_ids', 'attention_mask', 'liwc_features', 'label'])"
      ],
      "metadata": {
        "id": "UQmPoU-oDZho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer, IntervalStrategy\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "liwc_cols = [col for col in train_df.columns if col.startswith('lex_')]\n",
        "\n",
        "# 2. Define the safe metadata columns (Notice 'confidence' is excluded!)\n",
        "metadata_cols = [\n",
        "    'social_upvote_ratio', 'social_num_comments',\n",
        "    'syntax_fk_grade', 'sentiment',\n",
        "    'social_karma', 'syntax_ari'\n",
        "]\n",
        "\n",
        "# 3. Combine them into one master list\n",
        "all_numerical_cols = liwc_cols + metadata_cols\n",
        "\n",
        "# 4. Scale everything together to the [0, 1] range\n",
        "scaler = MinMaxScaler()\n",
        "X_num_scaled = scaler.fit_transform(train_df[all_numerical_cols])\n",
        "\n",
        "# Split the LIWC array using the EXACT SAME random_state as your text split!\n",
        "_, _, X_liwc_train, X_liwc_val = train_test_split(\n",
        "    X_text, X_num_scaled, test_size=0.2, stratify=y, random_state=SEED\n",
        ")\n",
        "\n",
        "# Safety check: Remove the column if it exists from a previous cell run\n",
        "if \"liwc_features\" in tokenized_train.column_names:\n",
        "    tokenized_train = tokenized_train.remove_columns(\"liwc_features\")\n",
        "if \"liwc_features\" in tokenized_val.column_names:\n",
        "    tokenized_val = tokenized_val.remove_columns(\"liwc_features\")\n",
        "\n",
        "# Add the correctly split LIWC arrays to your Hugging Face datasets\n",
        "tokenized_train = tokenized_train.add_column(\"liwc_features\", X_liwc_train.tolist())\n",
        "tokenized_val = tokenized_val.add_column(\"liwc_features\", X_liwc_val.tolist())\n",
        "\n",
        "# Format for PyTorch\n",
        "tokenized_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'liwc_features', 'label'])\n",
        "tokenized_val.set_format(type='torch', columns=['input_ids', 'attention_mask', 'liwc_features', 'label'])\n",
        "\n",
        "# The rewritten initialization\n",
        "num_liwc_cols = X_liwc_scaled.shape[1]\n",
        "hybrid_model = HybridDistilBertLIWC(num_liwc_features=num_liwc_cols)\n",
        "\n",
        "# Move model to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "hybrid_model.to(device)\n",
        "\n",
        "# Define the exact same training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./hybrid_results\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=IntervalStrategy.EPOCH,\n",
        "    save_strategy=IntervalStrategy.EPOCH,\n",
        "    load_best_model_at_end=True,\n",
        "    remove_unused_columns=False # CRITICAL: Prevents HF from deleting our custom 'liwc_features'\n",
        ")\n",
        "\n",
        "# Define the compute metrics function (from earlier)\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
        "    return {'accuracy': accuracy_score(labels, predictions), 'f1': f1}\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=hybrid_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train!\n",
        "print(\"Starting Hybrid Model Training...\")\n",
        "trainer.train()\n",
        "\n"
      ],
      "metadata": {
        "id": "T-9pIkfUEfgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Evaluate\n",
        "trainer.compute_metrics = compute_metrics\n",
        "detailed_metrics = trainer.evaluate()\n",
        "print(f\"DistilBERT F1-Score: {detailed_metrics['eval_f1']:.4f}\")\n",
        "print(f\"DistilBERT Accuracy: {detailed_metrics['eval_accuracy']:.4f}\")"
      ],
      "metadata": {
        "id": "c2ZVp06AHVUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 8 â€“ Deep Dive Analyses"
      ],
      "metadata": {
        "id": "CypsXvROTXfr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Dive 1: Subreddit-Level Performance\n"
      ],
      "metadata": {
        "id": "sjVh4JMzTZqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Dive 2: High-Confidence Failure Cases\n"
      ],
      "metadata": {
        "id": "K5L2-IMrTjqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Dive 3: Feature-Based Explanations (SHAP)"
      ],
      "metadata": {
        "id": "MP-YdAxzTk-n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 9 â€“ Ethics & Deployment Considerations"
      ],
      "metadata": {
        "id": "ktd_URITTo6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ethical Considerations\n",
        "\n",
        "False negatives may result in missed opportunities for support, while false positives\n",
        "risk over-surveillance or mislabeling benign expressions as distress.\n",
        "In a deployment context such as public mental health monitoring in Singapore,\n",
        "models should be used as decision-support tools rather than automated interventions.\n",
        "\n",
        "\n",
        "Deployment in a local context must adhere to Singapore's Model AI Governance Framework. Specifically, we must address explainability: if a user is flagged for high stress, the system must provide a 'human-readable' reason (using SHAP or coefficients) to avoid black-box decision making."
      ],
      "metadata": {
        "id": "DapLodLWTrWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 10 â€“ Conclusion"
      ],
      "metadata": {
        "id": "pI0zOPIMTw9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "This project demonstrates that stress signals can be detected from informal social media text\n",
        "with reasonable performance. However, model errors reveal the importance of context,\n",
        "community norms, and cautious deployment. Future work should emphasize human-in-the-loop\n",
        "systems and longitudinal analysis.\n"
      ],
      "metadata": {
        "id": "hltfRXk9Tzfx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Recommendation: > \"Our investigation reveals a trade-off between Interpretability and Robustness. While the Traditional ML + LIWC approach is superior for clinical 'Sense-making'â€”revealing that self-preoccupation (lex_liwc_i) and present-moment focus (focuspresent) are key stress markersâ€”it remains vulnerable to linguistic nuances like negation. Therefore, for a real-world deployment where accuracy is paramount, we recommend DistilBERT as the optimal successor to our baseline.\""
      ],
      "metadata": {
        "id": "AR4UgqlB5tON"
      }
    }
  ]
}